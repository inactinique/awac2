{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on web-pages-en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the default topic-modeling notebook proposed by dataiku.\n",
    "\n",
    "Please be carefull: We are playing here with the whole dataset. Vectorizing, for instance, will take several hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dataset loading <a id=\"setup\" />\n",
    "\n",
    "**This notebook requires the installation of the [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html#installation) package.**\n",
    "[See here for help with intalling python packages.](https://www.dataiku.com/learn/guide/code/python/install-python-packages.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import warnings                         # Disable some warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "#from dataiku import pandasutils as pdu\n",
    "import pandas as pd,  seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation,NMF\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the French and English corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load web-pages.csv as a dataframe\n",
    "web_pages = pd.read_csv('data/web-pages.csv') # add nrows = 1000 to load only the first 1000 lines.\n",
    "\n",
    "# We create a mask where the language column = en\n",
    "web_pages_mask_en = web_pages['language']=='en'\n",
    "web_pages_mask_fr = web_pages['language']=='fr'\n",
    "\n",
    "# We apply this mask to web_pages so that our new dataframe is web_pages_en\n",
    "web_pages_en = web_pages[web_pages_mask_en]\n",
    "web_pages_fr = web_pages[web_pages_mask_fr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'web_pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/90/j99p488148vcmjxgs16_mq0j42556r/T/ipykernel_19285/2878778116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we delete the web_pages dataframe as we don't need it anymore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mweb_pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'web_pages' is not defined"
     ]
    }
   ],
   "source": [
    "# we delete the web_pages dataframe as we don't need it anymore\n",
    "del web_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>mime_type_web_server</th>\n",
       "      <th>mime_type_tika</th>\n",
       "      <th>language</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200309</td>\n",
       "      <td>cdc.gov</td>\n",
       "      <td>https://www.cdc.gov/coronavirus/2019-ncov/inde...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>Coronavirus Disease 2019 (COVID-19) | CDC Skip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200309</td>\n",
       "      <td>cdc.gov</td>\n",
       "      <td>https://www.cdc.gov/coronavirus/2019-ncov/inde...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>Coronavirus Disease 2019 (COVID-19) | CDC Skip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200309</td>\n",
       "      <td>wwwnc.cdc.gov</td>\n",
       "      <td>https://wwwnc.cdc.gov/travel/page/covid-19-cru...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>COVID-19 and Cruise Ship Travel  | Travelers' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200309</td>\n",
       "      <td>api.addthis.com</td>\n",
       "      <td>https://api.addthis.com/oexchange/0.8/forward/...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>Missing Required Parameter Your request is mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20200309</td>\n",
       "      <td>addthis.com</td>\n",
       "      <td>https://www.addthis.com/tellfriend_v2.php?v=30...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>AddThis - Email a Friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699383</th>\n",
       "      <td>20200420</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>https://twitter.com/intent/tweet?text=&amp;url=htt...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>Share a link on Twitter Twitter Sign up Share ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699384</th>\n",
       "      <td>20200420</td>\n",
       "      <td>linkedin.com</td>\n",
       "      <td>https://www.linkedin.com/uas/login?session_red...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>LinkedIn Login, Sign in | LinkedIn LinkedIn We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699386</th>\n",
       "      <td>20200421</td>\n",
       "      <td>accounts.google.com</td>\n",
       "      <td>https://accounts.google.com/ServiceLogin?passi...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>Sign in - Google Accounts One account. All of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699387</th>\n",
       "      <td>20200421</td>\n",
       "      <td>plotly.com</td>\n",
       "      <td>https://plotly.com/</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>Modern Analytic Apps for the Enterprise - Plot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699391</th>\n",
       "      <td>20200421</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>https://www.facebook.com/login.php?next=https%...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>Facebook Facebook You must log in to continue....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3543144 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crawl_date               domain  \\\n",
       "0          20200309              cdc.gov   \n",
       "1          20200309              cdc.gov   \n",
       "2          20200309        wwwnc.cdc.gov   \n",
       "3          20200309      api.addthis.com   \n",
       "6          20200309          addthis.com   \n",
       "...             ...                  ...   \n",
       "6699383    20200420          twitter.com   \n",
       "6699384    20200420         linkedin.com   \n",
       "6699386    20200421  accounts.google.com   \n",
       "6699387    20200421           plotly.com   \n",
       "6699391    20200421         facebook.com   \n",
       "\n",
       "                                                       url  \\\n",
       "0        https://www.cdc.gov/coronavirus/2019-ncov/inde...   \n",
       "1        https://www.cdc.gov/coronavirus/2019-ncov/inde...   \n",
       "2        https://wwwnc.cdc.gov/travel/page/covid-19-cru...   \n",
       "3        https://api.addthis.com/oexchange/0.8/forward/...   \n",
       "6        https://www.addthis.com/tellfriend_v2.php?v=30...   \n",
       "...                                                    ...   \n",
       "6699383  https://twitter.com/intent/tweet?text=&url=htt...   \n",
       "6699384  https://www.linkedin.com/uas/login?session_red...   \n",
       "6699386  https://accounts.google.com/ServiceLogin?passi...   \n",
       "6699387                                https://plotly.com/   \n",
       "6699391  https://www.facebook.com/login.php?next=https%...   \n",
       "\n",
       "        mime_type_web_server         mime_type_tika language  \\\n",
       "0                  text/html              text/html       en   \n",
       "1                  text/html              text/html       en   \n",
       "2                  text/html              text/html       en   \n",
       "3                  text/html  application/xhtml+xml       en   \n",
       "6                  text/html  application/xhtml+xml       en   \n",
       "...                      ...                    ...      ...   \n",
       "6699383            text/html              text/html       en   \n",
       "6699384            text/html              text/html       en   \n",
       "6699386            text/html              text/html       en   \n",
       "6699387            text/html              text/html       en   \n",
       "6699391            text/html              text/html       en   \n",
       "\n",
       "                                                   content  \n",
       "0        Coronavirus Disease 2019 (COVID-19) | CDC Skip...  \n",
       "1        Coronavirus Disease 2019 (COVID-19) | CDC Skip...  \n",
       "2        COVID-19 and Cruise Ship Travel  | Travelers' ...  \n",
       "3        Missing Required Parameter Your request is mis...  \n",
       "6                                 AddThis - Email a Friend  \n",
       "...                                                    ...  \n",
       "6699383  Share a link on Twitter Twitter Sign up Share ...  \n",
       "6699384  LinkedIn Login, Sign in | LinkedIn LinkedIn We...  \n",
       "6699386  Sign in - Google Accounts One account. All of ...  \n",
       "6699387  Modern Analytic Apps for the Enterprise - Plot...  \n",
       "6699391  Facebook Facebook You must log in to continue....  \n",
       "\n",
       "[3543144 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We display our dataframe web_pages_en\n",
    "web_pages_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>mime_type_web_server</th>\n",
       "      <th>mime_type_tika</th>\n",
       "      <th>language</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>20200309</td>\n",
       "      <td>googletagmanager.com</td>\n",
       "      <td>https://www.googletagmanager.com/ns.html?id=GT...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>fr</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>20200310</td>\n",
       "      <td>oie.int</td>\n",
       "      <td>https://www.oie.int/fr/expertise-scientifique/...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>fr</td>\n",
       "      <td>Questions et réponses sur le COVID-19: OIE - W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20200310</td>\n",
       "      <td>fao.org</td>\n",
       "      <td>http://www.fao.org/2019-ncov/fr/</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>fr</td>\n",
       "      <td>Nouveau coronavirus (2019-nCoV) | FAO | Organi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>20200310</td>\n",
       "      <td>googletagmanager.com</td>\n",
       "      <td>https://www.googletagmanager.com/ns.html?id=GT...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>fr</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>20200310</td>\n",
       "      <td>merckmanuals.com</td>\n",
       "      <td>https://www.merckmanuals.com/fr-ca/accueil/new...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>fr</td>\n",
       "      <td>Épidémie due à un nouveau coronavirus chez l’h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698519</th>\n",
       "      <td>20200421</td>\n",
       "      <td>pasteur.fr</td>\n",
       "      <td>https://www.pasteur.fr/fr/nous-soutenir/covid-...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>fr</td>\n",
       "      <td>Covid-19 : comment soutenir les recherches de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698534</th>\n",
       "      <td>20200421</td>\n",
       "      <td>cps.ca</td>\n",
       "      <td>https://www.cps.ca/fr/search-recherche</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>fr</td>\n",
       "      <td>Résultats de la recherche | Société canadienne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698571</th>\n",
       "      <td>20200421</td>\n",
       "      <td>go.pardot.com</td>\n",
       "      <td>https://go.pardot.com/l/375732/2017-07-24/275x</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>fr</td>\n",
       "      <td>Email Comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698585</th>\n",
       "      <td>20200421</td>\n",
       "      <td>inspq.qc.ca</td>\n",
       "      <td>https://www.inspq.qc.ca/publications/2906-pci-...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>fr</td>\n",
       "      <td>COVID-19 : Mesures de prévention et de contrôl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698807</th>\n",
       "      <td>20200421</td>\n",
       "      <td>cibc.com</td>\n",
       "      <td>https://www.cibc.com/fr/business/covid-19/emer...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>fr</td>\n",
       "      <td>Compte d’urgence pour les entreprises canadien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254352 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crawl_date                domain  \\\n",
       "53         20200309  googletagmanager.com   \n",
       "81         20200310               oie.int   \n",
       "82         20200310               fao.org   \n",
       "200        20200310  googletagmanager.com   \n",
       "278        20200310      merckmanuals.com   \n",
       "...             ...                   ...   \n",
       "6698519    20200421            pasteur.fr   \n",
       "6698534    20200421                cps.ca   \n",
       "6698571    20200421         go.pardot.com   \n",
       "6698585    20200421           inspq.qc.ca   \n",
       "6698807    20200421              cibc.com   \n",
       "\n",
       "                                                       url  \\\n",
       "53       https://www.googletagmanager.com/ns.html?id=GT...   \n",
       "81       https://www.oie.int/fr/expertise-scientifique/...   \n",
       "82                        http://www.fao.org/2019-ncov/fr/   \n",
       "200      https://www.googletagmanager.com/ns.html?id=GT...   \n",
       "278      https://www.merckmanuals.com/fr-ca/accueil/new...   \n",
       "...                                                    ...   \n",
       "6698519  https://www.pasteur.fr/fr/nous-soutenir/covid-...   \n",
       "6698534             https://www.cps.ca/fr/search-recherche   \n",
       "6698571     https://go.pardot.com/l/375732/2017-07-24/275x   \n",
       "6698585  https://www.inspq.qc.ca/publications/2906-pci-...   \n",
       "6698807  https://www.cibc.com/fr/business/covid-19/emer...   \n",
       "\n",
       "        mime_type_web_server         mime_type_tika language  \\\n",
       "53                 text/html              text/html       fr   \n",
       "81                 text/html  application/xhtml+xml       fr   \n",
       "82                 text/html  application/xhtml+xml       fr   \n",
       "200                text/html              text/html       fr   \n",
       "278                text/html              text/html       fr   \n",
       "...                      ...                    ...      ...   \n",
       "6698519            text/html              text/html       fr   \n",
       "6698534            text/html              text/html       fr   \n",
       "6698571            text/html              text/html       fr   \n",
       "6698585            text/html              text/html       fr   \n",
       "6698807            text/html              text/html       fr   \n",
       "\n",
       "                                                   content  \n",
       "53                                                      ns  \n",
       "81       Questions et réponses sur le COVID-19: OIE - W...  \n",
       "82       Nouveau coronavirus (2019-nCoV) | FAO | Organi...  \n",
       "200                                                     ns  \n",
       "278      Épidémie due à un nouveau coronavirus chez l’h...  \n",
       "...                                                    ...  \n",
       "6698519  Covid-19 : comment soutenir les recherches de ...  \n",
       "6698534  Résultats de la recherche | Société canadienne...  \n",
       "6698571                                     Email Comments  \n",
       "6698585  COVID-19 : Mesures de prévention et de contrôl...  \n",
       "6698807  Compte d’urgence pour les entreprises canadien...  \n",
       "\n",
       "[254352 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We display our dataframe web_pages_fr\n",
    "web_pages_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, some fun with the english corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crawl_date', 'domain', 'url', 'mime_type_web_server', 'mime_type_tika',\n",
       "       'language', 'content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we display the columns so that we can choose the right one to process the topic modelling\n",
    "web_pages_en.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Coronavirus Disease 2019 (COVID-19) | CDC Skip...\n",
       "1          Coronavirus Disease 2019 (COVID-19) | CDC Skip...\n",
       "2          COVID-19 and Cruise Ship Travel  | Travelers' ...\n",
       "3          Missing Required Parameter Your request is mis...\n",
       "6                                   AddThis - Email a Friend\n",
       "                                 ...                        \n",
       "6699383    Share a link on Twitter Twitter Sign up Share ...\n",
       "6699384    LinkedIn Login, Sign in | LinkedIn LinkedIn We...\n",
       "6699386    Sign in - Google Accounts One account. All of ...\n",
       "6699387    Modern Analytic Apps for the Enterprise - Plot...\n",
       "6699391    Facebook Facebook You must log in to continue....\n",
       "Name: content, Length: 3543144, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We define the column that we will choose for the topic modelling\n",
    "raw_text_col_en = \"content\"\n",
    "# We create the dataframe with the \"content\" column only\n",
    "raw_text_en = web_pages_en[raw_text_col_en]\n",
    "# We remove the NaN to be sure\n",
    "raw_text_en = raw_text_en.dropna()\n",
    "# We display the result (to be sure we did what we wanted to do)\n",
    "raw_text_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "\n",
    "We cannot directly feed the text to the Topics Extraction Algorithms. We first need to process the text in order to get numerical vectors. We achieve this by applying either a CountVectorizer() or a TfidfVectorizer(). For more information on those technics, please refer to thid [sklearn documentation](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any text mining task, we first need to remove stop words that provide no useful information about topics. *sklearn* provides a default stop words list for english, but we can alway add to it any custom stop words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove the stopwords\n",
    "\n",
    "# Uncomment the next line to define custom stopwords\n",
    "#custom_stop_words = [u'did', u'good', u'right', u'said', u'does', u'way',u'edu', u'com', u'mail', u'thanks', u'post', u'address', u'university', u'email', u'soon', u'article',u'people', u'god', u'don', u'think', u'just', u'like', u'know', u'time', u'believe', u'say',u'don', u'just', u'think', u'probably', u'use', u'like', u'look', u'stuff', u'really', u'make', u'isn']\n",
    "\n",
    "stop_words_en = text.ENGLISH_STOP_WORDS #.union(custom_stop_words) - add this part if you added custom stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer() on the text data <a id=\"tfidf\" /> \n",
    "\n",
    "We first initialise a CountVectorizer() object and then apply the fit_transform method to the text.\n",
    "\n",
    "**This will take ages (several hours), be very patient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3543144, 1498847)\n"
     ]
    }
   ],
   "source": [
    "cnt_vectorizer_en = CountVectorizer(stop_words = stop_words_en,lowercase = True,\n",
    "                    token_pattern = r'\\b[a-zA-Z]{3,}\\b', max_df = 0.85, min_df = 2)\n",
    "\n",
    "text_cnt_en = cnt_vectorizer_en.fit_transform(raw_text_en)\n",
    "\n",
    "print(text_cnt_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer() on the text data <a id=\"tfidf\" /> \n",
    "\n",
    "We first initialise a TfidfVectorizer() object and then apply the fit_transform method to the text.\n",
    "\n",
    "**Same: ages, patience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3543144, 2527)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer_en = TfidfVectorizer(strip_accents = 'unicode',stop_words = stop_words_en,lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b', max_df = 0.75, min_df = 0.02)\n",
    "\n",
    "text_tfidf_en = tfidf_vectorizer_en.fit_transform(raw_text_en)\n",
    "\n",
    "print(text_tfidf_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will apply the topics extraction to `text_tidf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Extraction Models <a id=\"mod\" /> \n",
    "\n",
    "There are two very popular models for topic modelling, both available in the sklearn library: \n",
    "\n",
    "* [NMF (Non-negative Matrix Factorization)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization),\n",
    "* [LDA (Latent Dirichlet Allocation)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "Those two topic modeling algorithms infer topics from a collection of texts by viewing each document as a mixture of various topics. The only parameter we need to choose is the number of desired topics `n_topics`.  \n",
    "It is recommended to try different values for `n_topics` in order to find the most insightful topics. For that, we will show below different analyses (most frequent words per topics and heatmaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this line for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_model_en = LatentDirichletAllocation(n_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_model_en.fit(text_tfidf_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NMF instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_model_nmf_en = NMF(n_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics_model_nmf.fit(text_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Frequent Words per Topics\n",
    "An important way to assess the validity of our topic modelling is to directly look at the most frequent words in each topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_top_words = 25\n",
    "feature_names_en = tfidf_vectorizer_en.get_feature_names()\n",
    "\n",
    "def get_top_words_topic(topic_idx):\n",
    "    topic = topics_model.components_[topic_idx]\n",
    "   \n",
    "    print( [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] )\n",
    "    \n",
    "for topic_idx, topic in enumerate(topics_model.components_):\n",
    "    print (\"Topic #%d:\" % topic_idx )\n",
    "    get_top_words_topic(topic_idx)\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the words present, if some are very common you may want to go back to the [definition of custom stop words](#stop_words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Naming the topics\n",
    "\n",
    "Thanks to the above analysis, we can try to name each topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_topic_name = {i: \"topic_\"+str(i) for i in range(n_topics)}\n",
    "#dict_topic_name = my_dict_topic_name #Define here your own name mapping and uncomment this !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Heatmaps\n",
    "\n",
    "Another visual helper to better understand the found topics is to look at the heatmap for the document-topic and topic-words matrices. This gives us the distribution of topics over the collection of documents and the distribution of words over the topics.  \n",
    "We start with the topic-word heatmap where the darker the color is the more the word is representative of the topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = pd.DataFrame(topics_model.components_.T)\n",
    "word_model.index = feature_names\n",
    "word_model.columns.name = 'topic'\n",
    "word_model['norm'] = (word_model).apply(lambda x: x.abs().max(),axis=1)\n",
    "word_model = word_model.sort_values(by='norm',ascending=0) # sort the matrix by the norm of row vector\n",
    "word_model.rename(columns = dict_topic_name, inplace = True) #naming topic\n",
    " \n",
    "del word_model['norm']\n",
    "\n",
    "plt.figure(figsize=(25,20))\n",
    "sns.heatmap(word_model[:25]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now display the document-topic heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the document-topic matrix\n",
    "document_model = pd.DataFrame(topics_model.transform(text_tfidf))\n",
    "document_model.columns.name = 'topic'\n",
    "document_model.rename(columns = dict_topic_name, inplace = True) #naming topics\n",
    "\n",
    "plt.figure(figsize=(9,8))\n",
    "sns.heatmap(document_model.sort_index()[:100]) #we limit here to the first 10 texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic distribution over the corpus  \n",
    "We can look at how the topics are represented in the collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_proportion = document_model.sum()/document_model.sum().sum()\n",
    "topics_proportion.plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we can investigate the documents the most representative for the given topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_documents_topics(topic_name, n_doc = 3, excerpt = True):\n",
    "    '''This returns the n_doc documents most representative of topic_name'''\n",
    "    \n",
    "    document_index = list(document_model[topic_name].sort_values(ascending = False).index)[:n_doc]\n",
    "    for order, i in enumerate(document_index):\n",
    "        \"Text for the {}-th most representative document for topic {}:\\n\".format(order + 1,topic_name)\n",
    "        if excerpt:\n",
    "            raw_text[i][:1000]\n",
    "        else:\n",
    "            raw_text[i]\n",
    "        \"\\n******\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_documents_topics(\"topic_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Visualization with pyLDAvis <a id=\"viz\">\n",
    "\n",
    "Thanks to the pyLDAvis package, we can easily visualise and interpret the topics that has been fit to our corpus of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensimvis.prepare(topics_model, text_tfidf, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Clustering  <a id=\"clust\">  \n",
    "\n",
    "Once we have fitted topics on the text data, we can try to understand how they relate to one another: we achieve this by doing a hierachical clustering on the topics. We propose two methods, the first is based on a correlation table between topics, the second on a contigency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix between topics\n",
    "cor_matrix = np.corrcoef(document_model.iloc[:,:n_topics].values,rowvar=0)\n",
    "\n",
    "#Renaming of the index and columns\n",
    "cor_matrix = pd.DataFrame(cor_matrix)\n",
    "cor_matrix.rename(index = dict_topic_name, inplace = True)\n",
    "cor_matrix.rename(columns= dict_topic_name, inplace = True)\n",
    "\n",
    "sns.clustermap(cor_matrix, cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contingency table on the binarized document-topic matrix\n",
    "document_bin_topic = (document_model.iloc[:,:n_topics] > 0.25).astype(int)\n",
    "contingency_matrix = np.dot(document_bin_topic.T.values, document_bin_topic.values )\n",
    "\n",
    "#Renaming of the index and columns\n",
    "contingency_matrix = pd.DataFrame(contingency_matrix)\n",
    "contingency_matrix.rename(index = dict_topic_name, inplace = True)\n",
    "contingency_matrix.rename(columns= dict_topic_name, inplace = True)\n",
    "\n",
    "sns.clustermap(contingency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further steps  <a id=\"next\">  \n",
    "\n",
    "Topics extraction is a vast subject and a notebook can only show so much. There still much thing we could do, here are some ideas:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard documents from noise topics\n",
    "The following helper function takes as argument the topics for which we wish to discard documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_doc(*topic_name):\n",
    "    \n",
    "    doc_max_topic = document_model.idxmax(axis = 1)\n",
    "    \"Removing documents whose main topic are in \", topic_name\n",
    "    doc_max_topic_filtered = doc_max_topic[~doc_max_topic.isin(topic_name)]\n",
    "    return [raw_text[i] for i in doc_max_topic_filtered.index.tolist()]\n",
    "\n",
    "#E.g.: to remove documents whose main topic are topic_1 or topic_3, we would simply call remove_doc(\"topic_0\",\"topic_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 20newsgroup dataset, try this to remove text of topic \"Misc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_text_filtered = remove_doc(\"Misc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring the topic model on new text\n",
    "Finally, we can score new text with our topic model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = raw_text[:3] #Change this to the new text you'd like to score !\n",
    "\n",
    "tfidf_new_text = tfidf_vectorizer.transform(new_text)\n",
    "result = pd.DataFrame(topics_model.transform(tfidf_new_text), columns = [dict_topic_name[i] for i in range(n_topics)])\n",
    "sns.heatmap(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now for the French corpus (to be reviewed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So our dataframe is ready from the beginning. Here, let's view it just to be sure\n",
    "web_pages_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_col_fr = \"content\"\n",
    "raw_text_fr = df[raw_text_col_fr]\n",
    "raw_text_fr = raw_text_fr.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_fr = get_stop_words('french')\n",
    "#custom_stop_words = [u'did', u'good', u'right', u'said', u'does', u'way',u'edu', u'com', u'mail', u'thanks', u'post', u'address', u'university', u'email', u'soon', u'article',u'people', u'god', u'don', u'think', u'just', u'like', u'know', u'time', u'believe', u'say',u'don', u'just', u'think', u'probably', u'use', u'like', u'look', u'stuff', u'really', u'make', u'isn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vectorizer_fr = CountVectorizer(stop_words = stop_words_fr,lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b', max_df = 0.85, min_df = 2)\n",
    "\n",
    "text_cnt_fr = cnt_vectorizer_fr.fit_transform(raw_text_fr)\n",
    "\n",
    "print(text_cnt_fr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_fr = TfidfVectorizer(strip_accents = 'unicode',stop_words = stop_words_fr,lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b', max_df = 0.75, min_df = 0.02)\n",
    "\n",
    "text_tfidf_fr = tfidf_vectorizer_fr.fit_transform(raw_text_fr)\n",
    "\n",
    "print(text_tfidf_fr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics= 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_model_fr = LatentDirichletAllocation(n_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_model_fr.fit(text_tfidf_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 25\n",
    "feature_names_fr = tfidf_vectorizer_fr.get_feature_names()\n",
    "\n",
    "def get_top_words_topic(topic_idx):\n",
    "    topic = topics_model.components_[topic_idx]\n",
    "   \n",
    "    print( [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] )\n",
    "    \n",
    "for topic_idx, topic in enumerate(topics_model.components_):\n",
    "    print (\"Topic #%d:\" % topic_idx )\n",
    "    get_top_words_topic(topic_idx)\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "analyzedDataset": "AWAC2_webpage_FR",
  "createdOn": 1634445698186,
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "modifiedBy": "admin",
  "tags": [],
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
