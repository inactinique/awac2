{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN0gajhr7pKd"
   },
   "source": [
    "# Working with ARCH Derivatives\n",
    "\n",
    "In this notebook we'll do some analyses of the files we got from the COVID19 IIPC collection. It is based on the [original code from Nick Ruest and the Archive Unleashed team](https://github.com/archivesunleashed/notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OctPUqrG-K2W"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "In this section, we don't download the data, as it is too big. The data we are using should have been doanload prior and stored, in *.gz* files in a directory called 'data' which is in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw-HRGPhdKuk"
   },
   "source": [
    "Unzip the data. We unzip them here, as there seem to be a bug in macOS when you use the Apple provided tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VouhvD2bEBXf"
   },
   "outputs": [],
   "source": [
    "!gunzip data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3Nr_JCbsWx8"
   },
   "source": [
    "Let's check our `data` directory, and make sure they've downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RK74TCSuIXE",
    "outputId": "9f2cefc4-d90e-4a5f-8072-516a8b3d8daf"
   },
   "outputs": [],
   "source": [
    "!ls -1 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS0T6i4xPTVx"
   },
   "source": [
    "# Environment\n",
    "\n",
    "Next, we'll setup our environment so we can load our derivatives into [pandas](https://pandas.pydata.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgBtx0xFFv_i"
   },
   "source": [
    "# Data Table Display\n",
    "\n",
    "> We are working out of Google Colab. So this part is usable only if you use it within Colab. But with files that are that large, it seems illusory to work on Colab. I let here this code 'in case of'.\n",
    "\n",
    "Colab includes an extension that renders pandas dataframes into interactive displays that can be filtered, sorted, and explored dynamically. This can be very useful for taking a look at what each DataFrame provides!\n",
    "\n",
    "Data table display for pandas dataframes can be enabled by running:\n",
    "```python\n",
    "%load_ext google.colab.data_table\n",
    "```\n",
    "and disabled by running\n",
    "```python\n",
    "%unload_ext google.colab.data_table\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h2lJY2Z7xyt"
   },
   "outputs": [],
   "source": [
    "%load_ext google.colab.data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFAFb2X3_VJC"
   },
   "source": [
    "# Loading our ARCH Datasets as DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Next, we'll setup our datasets as pandas DataFrames to work with, and show a preview of each using the Data Table Display.\n",
    "\n",
    "Each block of derivative commands create a variable. That variable is a DataFrame with all of the information from a given derivative. After the DataFrame is created, a preview of it is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIq6rwtwb64d"
   },
   "source": [
    "## Collection\n",
    "\n",
    "A basic overview of the collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndZjsWKzUykd"
   },
   "source": [
    "### Domain Frequency\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* domain\n",
    "* count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "xUebRBMHuoRs",
    "outputId": "f345582a-63b2-4b1c-ee25-99dfd6e15b9d"
   },
   "outputs": [],
   "source": [
    "\n",
    "domain_frequency = pd.read_csv(\"data/domain-frequency.csv\")\n",
    "domain_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iox32X1fdYly"
   },
   "source": [
    "## Network\n",
    "These derivative files provide network graph data for analysis, and offer an opportunity to explore the way websites link to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtA20Erpdhna"
   },
   "source": [
    "### Domain Graph\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* source domain\n",
    "* target domain\n",
    "* count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "qrJHET-odk5Y",
    "outputId": "30dfdd68-8494-4ce1-d903-ccf58291ca6a"
   },
   "outputs": [],
   "source": [
    "domain_graph = pd.read_csv(\"data/domain-graph.csv\")\n",
    "domain_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3MBHq88dlZ2"
   },
   "source": [
    "### Image Graph\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* source of the image (where it was hosted)\n",
    "* the URL of the image\n",
    "* the alternative text of the image\n",
    "\n",
    "**Due to the size of the graph, this will take time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "AojTsVrfdn9n",
    "outputId": "2d1d5f03-06d1-4f51-9f65-25ae40e87516"
   },
   "outputs": [],
   "source": [
    "image_graph = pd.read_csv(\"data/image-graph.csv\")\n",
    "image_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijcDwS7tdpoG"
   },
   "source": [
    "### Web Graph\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* source\n",
    "* target\n",
    "* anchor text\n",
    "\n",
    "Note that this contains all links and is not aggregated into domains.\n",
    "\n",
    "**Due to the size of the graph, this will take time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9lm4jpumdpwU",
    "outputId": "e22b87fb-8a3a-4055-a3d8-84bd1928ea66"
   },
   "outputs": [],
   "source": [
    "web_graph = pd.read_csv(\"data/web-graph.csv\")\n",
    "web_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxZdqW7LbF8R"
   },
   "source": [
    "## File Formats\n",
    "\n",
    "These derivatives contain information on certain types of binary files found within a web archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtqbBhf-bHpR"
   },
   "source": [
    "### Audio\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* URL of the audio file\n",
    "* filename\n",
    "* audio extension\n",
    "* MIME type as provided by the web server\n",
    "* MIME type as detected by Apache TIKA\n",
    "* audio MD5 hash\n",
    "* audio SHA1 hash\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "oNk44pMZcSip",
    "outputId": "4934dd07-cebe-46ad-ccce-05fc9900ed0a"
   },
   "outputs": [],
   "source": [
    "audio = pd.read_csv(\"data/audio-information.csv\")\n",
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnEntLU0U2ox"
   },
   "source": [
    "### Images\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* URL of the image\n",
    "* filename\n",
    "* image extension\n",
    "* MIME type as provided by the web server\n",
    "* MIME type as detected by Apache TIKA\n",
    "* image width\n",
    "* image height\n",
    "* image MD5 hash\n",
    "* image SHA1 hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "GhE_Vhv8Awkx",
    "outputId": "7be5489a-c718-4845-fc70-b6cf42b66c04"
   },
   "outputs": [],
   "source": [
    "images = pd.read_csv(\"data/image-information.csv\")\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6USg-a-Rcaod"
   },
   "source": [
    "### PDFs\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* URL of the PDF file\n",
    "* filename\n",
    "* PDF extension\n",
    "* MIME type as provided by the web server\n",
    "* MIME type as detected by Apache TIKA\n",
    "* PDF MD5 hash\n",
    "* PDF SHA1 hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "Nmjfux3ucayR",
    "outputId": "a00bbeac-20d9-4138-9cff-490fa658db3a"
   },
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(\"data/pdf-information.csv\")\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owAkSikAcrvU"
   },
   "source": [
    "### PowerPoint (all software types) information\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* URL of a PowerPoint or similar file\n",
    "* filename\n",
    "* PowerPoint or similar file extension\n",
    "* MIME type as provided by the web server\n",
    "* MIME type as detected by Apache TIKA\n",
    "* PowerPoint or similar file MD5 hash\n",
    "* PowerPoint or similar file SHA1 hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "-Y0kGxvycvtN",
    "outputId": "621ba6b0-bdf3-4aa5-f998-e524fe118fa2"
   },
   "outputs": [],
   "source": [
    "powerpoint = pd.read_csv(\"data/powerpoint-information.csv\")\n",
    "powerpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOhl9xqOc1tq"
   },
   "source": [
    "### Spreadsheets\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* URL of the spreadsheet file\n",
    "* filename\n",
    "* spreadsheet extension\n",
    "* MIME type as provided by the web server\n",
    "* MIME type as detected by Apache TIKA\n",
    "* spreadsheet MD5 hash\n",
    "* spreadsheet SHA1 hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "-9MXi7g0c2BS",
    "outputId": "7710bca9-ea3b-4e5d-f93a-ed190cf42477"
   },
   "outputs": [],
   "source": [
    "spreadsheet = pd.read_csv(\"data/spreadsheet-information.csv\")\n",
    "spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmJP4rckdBq4"
   },
   "source": [
    "### Videos\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* URL of the video file\n",
    "* filename\n",
    "* video extension\n",
    "* MIME type as provided by the web server\n",
    "* MIME type as detected by Apache TIKA\n",
    "* video MD5 hash\n",
    "* video SHA1 hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "mKHMh6M-dB0J",
    "outputId": "d3cfa40a-35e0-4b15-a4bb-be06f6326d43"
   },
   "outputs": [],
   "source": [
    "video = pd.read_csv(\"data/video-information.csv\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3uYwccVdIqR"
   },
   "source": [
    "### Word Documents (all software types)\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* URL of the word document or similar file\n",
    "* filename\n",
    "* word document or similar file extension\n",
    "* MIME type as provided by the web server\n",
    "* MIME type as detected by Apache TIKA\n",
    "* word document or similar file MD5 hash\n",
    "* word document or similar file SHA1 hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "pXuBAaNzdIzc",
    "outputId": "675a3507-4702-4c54-e405-bd0254f43023"
   },
   "outputs": [],
   "source": [
    "word = pd.read_csv(\"data/word-document-information.csv\")\n",
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuzuazApycSu"
   },
   "source": [
    "## Text\n",
    "\n",
    "This derivative provides access to the \"plain text\" of a collection, extracted from web page HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GTL6VtBydje"
   },
   "source": [
    "### Web Pages\n",
    "\n",
    "Provides the following columns:\n",
    "\n",
    "* crawl date\n",
    "* web domain\n",
    "* URL\n",
    "* MIME type as provided by the web server\n",
    "* MIME type as detected by Apache TIKA\n",
    "* content (HTTP headers and HTML removed)\n",
    "\n",
    "**This file is HUGE! Be patient!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "jH5fqM9Tyfj7",
    "outputId": "02183875-fde4-4dc0-8f70-6d6c112ed13e"
   },
   "outputs": [],
   "source": [
    "web_pages = pd.read_csv(\"data/web-pages.csv\")\n",
    "web_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HPwOCNAvqMe"
   },
   "source": [
    "# Data Analysis\n",
    "\n",
    "Now that we have all of our datasets loaded up, we can begin to work with them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6Pkg0prv3BE"
   },
   "source": [
    "## Counting total files, and unique files\n",
    "\n",
    "Let's take a quick look at how to count items in DataFrames, and use total and unique files as an example to work with.\n",
    "\n",
    "It's definitely work checking out the [pandas documentation](https://pandas.pydata.org/docs/index.html). There are a lot of good examples available, along with a robust [API reference](https://pandas.pydata.org/docs/reference/index.html#api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFX4Gl3wv7bi"
   },
   "source": [
    "\n",
    "#### How many images are in this collection?\n",
    "\n",
    "We can take our `images` variable try a couple of functions to get the same answer.\n",
    "\n",
    "1.   `len(images.index)`\n",
    "  * Get the length of the DataFrame's index.\n",
    "2.   `images.shape[0]`\n",
    "  * Get the shape or dimensionality of the DataFrame, and take the first item in the tuple.\n",
    "3.  `images.count()`\n",
    "  * Count the number of rows for each column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTv8Oet3jiTH",
    "outputId": "b3c5db59-0920-4f4b-c838-efb55c36fe91"
   },
   "outputs": [],
   "source": [
    "len(images.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rYEERnTjifk",
    "outputId": "68d7d3c4-49e6-416d-a0c2-f05db1e53cb6"
   },
   "outputs": [],
   "source": [
    "images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bn-1v127aKIG",
    "outputId": "3c248b21-ac6c-4cc5-ead5-db0b71efed50"
   },
   "outputs": [],
   "source": [
    "images.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38veKiPhwKo4"
   },
   "source": [
    " #### How many unique images are in the collection?\n",
    "\n",
    " We can see if an image is unique or not by computing an [MD5 hash](https://en.wikipedia.org/wiki/MD5#MD5_hashes) of it, and comparing them. The exact same image might have a filename of `example.jpg` or `foo.jpg`. If the hash is computed for each, we can see that even with different file names, they are actually the same image. So, since we have both a `MD5` and `SHA1` hash column available in our DataFrame, we can just find the unique values, and count them!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WesM3kQowM5B",
    "outputId": "645d661a-b0a1-4cb9-af66-47df5d1b97a7"
   },
   "outputs": [],
   "source": [
    "len(images.md5.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIXkI0-1wWQf"
   },
   "source": [
    "#### What are the top 10 most occurring images in the collection?\n",
    "\n",
    "Here we can take advantage of [`value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html) to provide us with a list of MD5 hashes, and their respective counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ts03OFyjPIM",
    "outputId": "bf9870c7-40e5-474c-b38a-d35aa4a4c3f6"
   },
   "outputs": [],
   "source": [
    "images[\"md5\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG7pGZUEwlaI"
   },
   "source": [
    "\n",
    "#### What's the information around all of the occurances of `d89746888da2d9510b64a9f031eaecd5`?\n",
    "\n",
    "What, you mean you don't know what `d89746888da2d9510b64a9f031eaecd5` means? \n",
    "\n",
    "Let's find those images in the DataFrame. We can here see some of the filenames used, it's dimensions, and it's URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "msmmm65lkSIK",
    "outputId": "e9227287-108a-4d53-8a24-91bf1a665647"
   },
   "outputs": [],
   "source": [
    "images.loc[images[\"md5\"] == \"d89746888da2d9510b64a9f031eaecd5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE-8epARIG0-"
   },
   "source": [
    "### What does `377d257f2d2e294916143c069141c1c5` look like?\n",
    "\n",
    "Let's grab the live web URL for the image, and then see if we can display it in a markdown cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wB3VqcmgJQM0",
    "outputId": "67a36914-60fe-485e-c592-a1b777c5925a"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "one_image = images.loc[images[\"md5\"] == \"377d257f2d2e294916143c069141c1c5\"].head(1)\n",
    "one_image[\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YV2u_8aLEJS"
   },
   "source": [
    "![377d257f2d2e294916143c069141c1c5](https://analytics.twitter.com/i/adsct?txn_id=l4o6d&p_id=Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YfsUGSRt1Ns"
   },
   "source": [
    "Well, yes, this is a transparent GIF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrDaY-C8iwhl"
   },
   "source": [
    "\n",
    "Another point of examination with the `images` DataFrame is the `height` and `width` columns. You could take a look at the largest images, or even `0x0` images, and potentially `spacer.gif` occurrences!\n",
    "\n",
    "* “[The invention and dissemination of the spacer gif: implications for the future of access and use of web archives](https://link.springer.com/article/10.1007/s42803-019-00006-8)”\n",
    "* \"[GeoCities and the spacer.gif](https://ruebot.net/post/geocities-and-the-spacer-gif/)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbLLZW2awzCv"
   },
   "source": [
    "#### What are the top 10 most occuring filenames in the collection?\n",
    "\n",
    "Note that this is of course different than the MD5 results up above. Here we are focusing _just_ on filename. So `cover.jpg` for example, might actually be referring to different images who happen to have the same name.\n",
    "\n",
    "Here we can use `value_counts()` again, but this time we'll create a variable for the top filenames so we can use it later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQaw54ACkwdZ",
    "outputId": "ff03cbcc-6886-4b74-fc62-8a821598f9cf"
   },
   "outputs": [],
   "source": [
    "top_filenames = images[\"filename\"].value_counts().head(10)\n",
    "top_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7F3re20BQRI"
   },
   "source": [
    "#### Let's create our first graph!\n",
    "\n",
    "We'll plot the data first with pandas [plot](https://pandas.pydata.org/docs/reference/api/pandas.Series.plot.html) functionality, and then plot the data with [Altair](https://altair-viz.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "sRvlstfsBWEZ",
    "outputId": "f984304d-1e05-488b-9fa8-09024970ae98"
   },
   "outputs": [],
   "source": [
    "top_filenames_chart = top_filenames.plot.bar(figsize=(25, 10))\n",
    "\n",
    "top_filenames_chart.set_title(\"Top Filenames\", fontsize=22)\n",
    "top_filenames_chart.set_xlabel(\"Filename\", fontsize=20)\n",
    "top_filenames_chart.set_ylabel(\"Count\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQgeOObvgLvK"
   },
   "source": [
    "Now let's setup Altair, and plot the data with Altair. Altair is useful for creating vizualization since they can be easily exported as a PNG or SVG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7Z4J6qjWaVM"
   },
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "id": "s0xwvILYWkgg",
    "outputId": "6d8f7eb2-63d1-4a8d-ed13-532396b4996c"
   },
   "outputs": [],
   "source": [
    "top_filenames_altair = (\n",
    "    images[\"filename\"]\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    "    .rename_axis(\"Filename\")\n",
    "    .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "filenames_bar = (\n",
    "    alt.Chart(top_filenames_altair)\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"Filename:O\", sort=\"-y\"), y=alt.Y(\"Count:Q\"))\n",
    ")\n",
    "\n",
    "filenames_rule = (\n",
    "    alt.Chart(top_filenames_altair).mark_rule(color=\"red\").encode(y=\"mean(Count):Q\")\n",
    ")\n",
    "\n",
    "\n",
    "filenames_text = filenames_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
    "    text=\"Count:Q\"\n",
    ")\n",
    "\n",
    "(filenames_bar + filenames_rule + filenames_text).properties(\n",
    "    width=1400, height=700, title=\"Top Filenames\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BneaN9cgGoly"
   },
   "source": [
    "#### How about a file format distribution?\n",
    "\n",
    "What _kind_ of image files are present? We can discover this by checking their \"media type\", or [MIME type](https://en.wikipedia.org/wiki/Media_type). \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "RDd-J8D-GwDk",
    "outputId": "589699f2-77f7-4323-998e-a82517181e7f"
   },
   "outputs": [],
   "source": [
    "image_mime_types = (\n",
    "    images[\"mime_type_tika\"]\n",
    "    .value_counts()\n",
    "    .head(5)\n",
    "    .rename_axis(\"MIME Type\")\n",
    "    .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "image_mimes_bar = (\n",
    "    alt.Chart(image_mime_types)\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"MIME Type:O\", sort=\"-y\"), y=alt.Y(\"Count:Q\"))\n",
    ")\n",
    "\n",
    "image_mime_rule = (\n",
    "    alt.Chart(image_mime_types).mark_rule(color=\"red\").encode(y=\"mean(Count):Q\")\n",
    ")\n",
    "\n",
    "image_mime_text = image_mimes_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
    "    text=\"Count:Q\"\n",
    ")\n",
    "\n",
    "(image_mimes_bar + image_mime_rule + image_mime_text).properties(\n",
    "    width=1400, height=700, title=\"Image File Format Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKqzuDVz-GiF"
   },
   "source": [
    "## Let's take a look at the domain frequency derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGHnjGRQPzqV"
   },
   "source": [
    "#### What does the distribution of domains look like?\n",
    "\n",
    "Here we can see which domains are the most frequent within the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "C_X_YSD4PyQi",
    "outputId": "3e3deab3-ed7e-4415-8eaf-1117b3756c68"
   },
   "outputs": [],
   "source": [
    "top_domains = domain_frequency.sort_values(\"count\", ascending=False).head(10)\n",
    "\n",
    "top_domains_bar = (\n",
    "    alt.Chart(top_domains)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=alt.X(\"domain:O\", title=\"Domain\", sort=\"-y\"),\n",
    "        y=alt.Y(\"count:Q\", title=\"Count, Mean of Count\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "top_domains_rule = (\n",
    "    alt.Chart(top_domains).mark_rule(color=\"red\").encode(y=\"mean(count):Q\")\n",
    ")\n",
    "\n",
    "top_domains_text = top_domains_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
    "    text=\"count:Q\"\n",
    ")\n",
    "\n",
    "(top_domains_bar + top_domains_rule + top_domains_text).properties(\n",
    "    width=1400, height=700, title=\"Domains Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDXDhqCcyyFj"
   },
   "source": [
    "### Top Level Domain Analysis\n",
    "\n",
    "pandas allows you to create new columns in a DataFrame based off of existing data. This comes in handy for a number of use cases with the available data that we have. In this case, let's create a new column, `tld`, which is based off an existing column, 'domain'. This example should provide you with an implementation pattern for expanding on these datasets to do further research and analysis.\n",
    "\n",
    "A [top-level domain](https://en.wikipedia.org/wiki/Top-level_domain) refers to the highest domain in an address - i.e. `.ca`, `.com`, `.org`, or yes, even `.pizza`.\n",
    "\n",
    "Things get a bit complicated, however, in some national TLDs. While `qc.ca` (the domain for Quebec) isn't really a top-level domain, it has many of the features of one as people can directly register under it. Below, we'll use the command `suffix` to include this. \n",
    "\n",
    "> You can learn more about suffixes at https://publicsuffix.org.\n",
    "\n",
    "We'll take the `domain` column and extract the `tld` from it with [`tldextract`](https://github.com/john-kurkowski/tldextract).\n",
    "\n",
    "First we'll add the [`tldextract`](https://github.com/john-kurkowski/tldextract) library to the notebook. Then, we'll create the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clPJuQAe5mcg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "mv7a-MLIx-3f",
    "outputId": "cd120ca9-39f4-4294-a00f-e466539907ee"
   },
   "outputs": [],
   "source": [
    "import tldextract\n",
    "\n",
    "domain_frequency[\"tld\"] = domain_frequency.apply(\n",
    "    lambda row: tldextract.extract(row.domain).suffix, axis=1\n",
    ")\n",
    "domain_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdXFS2yu8XYG"
   },
   "source": [
    "#### Next, let's count the distict TLDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lViQIU48e-u",
    "outputId": "caf97710-ff09-4bcf-fdd9-9e1c93af34e4"
   },
   "outputs": [],
   "source": [
    "tld_count = domain_frequency[\"tld\"].value_counts()\n",
    "tld_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm_V_0PGzZut"
   },
   "source": [
    "#### Next, we'll plot the TLD count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "id": "N8yNlOa-zmBD",
    "outputId": "eb39d031-d35b-44de-a4c9-47472940f421"
   },
   "outputs": [],
   "source": [
    "tld_count = (\n",
    "    domain_frequency[\"tld\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"TLD\")\n",
    "    .reset_index(name=\"Count\")\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "tld_bar = (\n",
    "    alt.Chart(tld_count)\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"TLD:O\", sort=\"-y\"), y=alt.Y(\"Count:Q\"))\n",
    ")\n",
    "\n",
    "tld_rule = alt.Chart(tld_count).mark_rule(color=\"red\").encode(y=\"mean(Count):Q\")\n",
    "\n",
    "tld_text = tld_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(text=\"Count:Q\")\n",
    "\n",
    "(tld_bar + tld_rule + tld_text).properties(\n",
    "    width=1400, height=700, title=\"Top Level Domain Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLriRk2UFlFP"
   },
   "source": [
    "## Web Crawl Frequency\n",
    "\n",
    "Let's see what the crawl frequency looks like by examining the `web_pages` DataFrame. First we'll create a new DataFrame by extracting the `crawl_date` and `domain` columns, and count the occurances of each domain and date combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "_gGjFO6DAOYT",
    "outputId": "e594cbae-d664-4d55-ec53-227173d71708"
   },
   "outputs": [],
   "source": [
    "crawl_sites = web_pages[[\"crawl_date\", \"domain\"]]\n",
    "crawl_sites = crawl_sites.value_counts().reset_index()\n",
    "crawl_sites.columns = [\"Date\", \"Site\", \"Count\"]\n",
    "crawl_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCJLNokxGaBk"
   },
   "source": [
    "Next, we'll create a stacked bar chart where each bar will show the distribution of pages in that crawl by top-level domain.\n",
    "\n",
    "**NOTE**: Charts like this one work a lot better with collections that have more than 1 or 2 crawl dates. The temporal aspect is definitely something to take into consideration with each of the examples provided in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "xQAx6JpNDL-I",
    "outputId": "dd292885-30a0-463a-93d7-9edf5fd95677"
   },
   "outputs": [],
   "source": [
    "## Altair has a default limit of 5000 rows, and this DataFrame is ~7700 rows, so we're going to disable the max allowed rows.\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "crawl_chart = (\n",
    "    alt.Chart(crawl_sites)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=\"Date:O\",\n",
    "        y=\"Count:Q\",\n",
    "        color=\"Site\",\n",
    "        tooltip=\"Site\",\n",
    "        order=alt.Order(\"Site\", sort=\"descending\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "crawl_chart.properties(width=1400, height=700, title=\"Web Crawl Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_PLHmrXZR34"
   },
   "source": [
    "## Examining the Domain Graph\n",
    "\n",
    "Remember the hyperlink Domain graph? Let's look at the web graph columns again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "rBTHCPjWZlWV",
    "outputId": "e294992e-4247-4827-8721-4572ecd8ba72"
   },
   "outputs": [],
   "source": [
    "domain_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6j_2QrjZtd3"
   },
   "source": [
    "\n",
    "### What are the most frequent `source` and `target` combinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "zOC5DePocC4c",
    "outputId": "39a609ba-c6dd-43d3-d32d-d1e9e4b3ad39"
   },
   "outputs": [],
   "source": [
    "top_links = domain_graph[[\"source\", \"target\"]].value_counts().head(150).reset_index()\n",
    "top_links.columns = [\"source\", \"target\", \"count\"]\n",
    "top_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UceDp9wC4CWt"
   },
   "source": [
    "## Can we create a network graph visualization with the data we have?\n",
    "\n",
    "Yes! We can take advantage [NetworkX](https://networkx.org/documentation/stable/index.html) to create some basic graphs.\n",
    "\n",
    "NetworkX is *really* powerful, so there is a lot more that can be done with it than what we're demonstrating here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPkV2asa4hXr"
   },
   "source": [
    "First we'll import `networkx` as well as `matplotlib.pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5axvp0L7OrhE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_jSty3K4taF"
   },
   "source": [
    "We can take advantage of [`from_pandas_edgelist`](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html) here since our three graph derivatives are edge tables, and initialize our graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoYrzuutOtru"
   },
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(\n",
    "    top_links, source=\"source\", target=\"target\", edge_key=\"target\", edge_attr=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GXQAt9X5JIt"
   },
   "source": [
    "Setup our graph, and draw it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "id": "AkM5xZe83fJe",
    "outputId": "d3810fef-a294-4ee8-8ad5-3da6a4051c0e"
   },
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G, k=25)\n",
    "options = {\n",
    "    \"node_size\": 1000,\n",
    "    \"node_color\": \"#bc5090\",\n",
    "    \"node_shape\": \"o\",\n",
    "    \"alpha\": 0.5,\n",
    "    \"linewidths\": 4,\n",
    "    \"font_size\": 10,\n",
    "    \"font_color\": \"black\",\n",
    "    \"width\": 2,\n",
    "    \"edge_color\": \"grey\",\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, **options)\n",
    "\n",
    "labels = {e: G.edges[e][\"count\"] for e in G.edges}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTcTLMHmTGXo"
   },
   "source": [
    "## Text Analysis\n",
    "\n",
    "Next, we'll do some basic text analysis with our `web_pages` DataFrame with `nltk` and`spaCy`, and end with a word cloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPzVa3WhTImQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QkkITU2TkkB",
    "outputId": "54c9b75b-e352-49d8-ac51-30af12196267"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x1k3fmHu1NU"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvmymu758duP"
   },
   "source": [
    "We'll drop the `NaN` values in our DataFrame to clean things up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nBEVNyMxjtHg",
    "outputId": "8effbdf9-7273-415f-9ff9-739ffb7fe6bf"
   },
   "outputs": [],
   "source": [
    "web_pages = web_pages.dropna()\n",
    "web_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dArfB41Vp5Rx"
   },
   "source": [
    "We need to set the [`mode.chained_assignment`](https://pandas.pydata.org/docs/user_guide/options.html?highlight=chained_assignment) to `None` now to silence some exception errors that will come up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UelY00RWxs_-"
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCg-yGKl8ne8"
   },
   "source": [
    "Next, we'll setup a tokenizer which will split on words, and create a new column which is the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwtFa9FfzLjC"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGSTGsdHr0l3"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokenized\"] = web_pages[\"content\"].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e4qcA8p85Od"
   },
   "source": [
    "Now well create a column with the tokenized value count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOXKqykU0Vgc"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"] = web_pages[\"content_tokenized\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1LvlTd-9GC9"
   },
   "source": [
    "### Basic word count statistics with pandas!\n",
    "\n",
    "Now we can use the power of pandas [Statisitcal functions](https://pandas.pydata.org/docs/user_guide/computation.html) to show us some basic statistics about the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wvUcbx29M4n"
   },
   "source": [
    "**Mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwO0KHnS9JJQ",
    "outputId": "363628e4-bef4-4f65-fc2f-c30af1d39a11"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idMyBDrU9V3O"
   },
   "source": [
    "**Standard deviation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-V6zi079WNv",
    "outputId": "f6189597-b524-4d3b-c45e-0948d342f30f"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai0QVx9t9bnt"
   },
   "source": [
    "**Max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbXjWwW99bzr",
    "outputId": "fa614c0c-6513-479c-982d-ace92d18f657"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLT5HLXM9f8W"
   },
   "source": [
    "**Min**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "629g31k39gH2",
    "outputId": "02ab8f30-8fd6-413e-d656-8c6a3064341f"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-bIw1Pu2NkW"
   },
   "source": [
    "### Pages with most words\n",
    "\n",
    "Let's create a bar chart that shows the pages with the most words. Here we can see the power of pandas at work, in terms of both analysis and visualization.\n",
    "\n",
    "First, let's show the query to get the data for our chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oktgvxIr2cSV"
   },
   "outputs": [],
   "source": [
    "word_count = (\n",
    "    web_pages[[\"url\", \"content_tokens\"]]\n",
    "    .sort_values(by=\"content_tokens\", ascending=False)\n",
    "    .head(25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "id": "3Eeserah2lBX",
    "outputId": "d4afc5b2-da15-4f07-eb1b-b913beb73f6b"
   },
   "outputs": [],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uepwHXS59tz-"
   },
   "source": [
    "Next, let's create a bar chart of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "ns1ZlwbnqoDi",
    "outputId": "6edfb7cf-29f5-4ff9-e2a9-07838a7784ac"
   },
   "outputs": [],
   "source": [
    "word_count_bar = (\n",
    "    alt.Chart(word_count)\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"url:O\", sort=\"-y\"), y=alt.Y(\"content_tokens:Q\"))\n",
    ")\n",
    "\n",
    "word_count_rule = (\n",
    "    alt.Chart(word_count).mark_rule(color=\"red\").encode(y=\"mean(content_tokens):Q\")\n",
    ")\n",
    "\n",
    "word_count_text = word_count_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
    "    text=\"content_tokens:Q\"\n",
    ")\n",
    "\n",
    "(word_count_bar + word_count_rule + word_count_text).properties(\n",
    "    width=1400, height=700, title=\"Pages with the most words\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhLIldv-3SOm"
   },
   "source": [
    "### How about NER on the page with the most tokens?\n",
    "\n",
    "[Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition), or NER, is an exciting field of natural language processing that lets us extract \"entities\" out of text; the names of people, locations, or organizations.\n",
    "\n",
    "To do this, we first need to find the pages that have the most tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Svg3ruY3cQg",
    "outputId": "aaa5f18d-82b8-46bd-c9f9-c240d0450ccb"
   },
   "outputs": [],
   "source": [
    "word_count_max = (\n",
    "    web_pages[[\"url\", \"content_tokens\", \"content\"]]\n",
    "    .sort_values(by=\"content_tokens\", ascending=False)\n",
    "    .head(1)\n",
    ")\n",
    "word_count_max[\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvufosIP-7JH"
   },
   "source": [
    "We'll remove the column width limit so we can check out our content for the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gLaHabY-vRM"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBxWH3MV_Gp-"
   },
   "source": [
    "Let's take a look at our page's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "5WykQM1H3YIU",
    "outputId": "3d033c1b-cc1f-461f-a95f-c8ec4d72ab47"
   },
   "outputs": [],
   "source": [
    "page = word_count_max[\"content\"].astype(\"unicode\").to_string()\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVEwDKpR5QcB"
   },
   "source": [
    "\n",
    "#### Setup spaCy\n",
    "\n",
    "We now need to set up [spaCy](https://en.wikipedia.org/wiki/SpaCy), a natural-language processing toolkit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D8tBLZz3wBp"
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.max_length = 1100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIPHPYyIEEwN"
   },
   "source": [
    "Next we'll run the natual language processor from SpaCy, and then display the NER output. Watch how it finds organizations, people, and beyond!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7LNVyHX633qT",
    "outputId": "7e4815f4-2d45-4763-df2a-c871f27dc0c0"
   },
   "outputs": [],
   "source": [
    "ner = nlp(page)\n",
    "displacy.render(ner, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtGdA7jjkmHg"
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH3vFgIHk2tB"
   },
   "source": [
    "We'll be using the [vaderSentiment](https://github.com/cjhutto/vaderSentiment) library, and [adapting examples](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html#) from Melanie Walsh's [\"Introduction to Cultural Analytics & Python\"](https://melaniewalsh.github.io/Intro-Cultural-Analytics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFx6dex-kuiY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LTJDMRUlIP6"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER\n",
    "sentimentAnalyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvST6DPxmxbI"
   },
   "source": [
    "We'll create a function, that we'll then apply to a DataFrame to create sentiment analysis scores for the `content` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xs8w1J5GmDEQ"
   },
   "outputs": [],
   "source": [
    "def calculate_sentiment(text):\n",
    "    # Run VADER on the text\n",
    "    scores = sentimentAnalyser.polarity_scores(text)\n",
    "    # Extract the compound score\n",
    "    compound_score = scores['compound']\n",
    "    # Return compound score\n",
    "    return compound_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g33U8Dflp9an"
   },
   "source": [
    "Since it will take some time to run the sentiment analysis on the entire `web_pages` DataFrame, we'll create a sample from `web_pages`, and run the sentiment analysis on the for demostration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zCsMvUdqRXy"
   },
   "outputs": [],
   "source": [
    "web_pages_sample = web_pages.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFKu-qksl4kC"
   },
   "outputs": [],
   "source": [
    "web_pages_sample['sentiment_score'] = web_pages_sample['content'].apply(calculate_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1XrTYhRrEqN"
   },
   "source": [
    "Let's see what the the scores look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2455
    },
    "id": "WVZXHG0Nq_TS",
    "outputId": "7a30faf8-594d-43cc-8a21-c2d8c5d4bd3e"
   },
   "outputs": [],
   "source": [
    "web_pages_sample[['sentiment_score', 'content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uziox47r9Ld"
   },
   "source": [
    "Finally, let's plot the sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "id": "d2wLS4KEriy_",
    "outputId": "3084db9c-f09c-4f2c-fdf5-852be443b80b"
   },
   "outputs": [],
   "source": [
    "sentiment_scores = web_pages_sample[['sentiment_score']].value_counts().head(10).reset_index()\n",
    "sentiment_scores = sentiment_scores.rename({\"sentiment_score\": \"Sentiment Score\", 0: \"Count\"}, axis=1)\n",
    "\n",
    "sentiment_chart = (\n",
    "    alt.Chart(sentiment_scores)\n",
    "    .mark_circle()\n",
    "    .encode(\n",
    "        x=alt.X(\"Sentiment Score:Q\", bin=True),\n",
    "        y=alt.Y(\"Count:Q\", bin=True),\n",
    "        size='Count')\n",
    ")\n",
    "\n",
    "sentiment_chart.properties(\n",
    "    width=1400, height=700, title=\"Sentiment Score Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-KLFb415e1y"
   },
   "source": [
    "### Wordcloud\n",
    "\n",
    "What better way to wrap-up this notebook than create a word cloud!\n",
    "\n",
    "Word clouds are always fun, right?! They're an interesting way to visualize word frequency, as the more times that a word occurs, the larger it will appear in the word cloud.\n",
    "\n",
    "Let's setup some dependencies here. We will install the [word_cloud](https://github.com/amueller/word_cloud) library, and setup some stop words via `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiS7nhq2eirZ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvKSPMvE95m_"
   },
   "source": [
    "Let's remove the remove the stopwords from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E06S_xO5-7c"
   },
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbcXX78s6pFH"
   },
   "outputs": [],
   "source": [
    "web_pages[\"stopwords\"] = web_pages[\"content_tokenized\"].apply(\n",
    "    lambda x: [item.lower() for item in x if item not in stopwords]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6IrEAcTEorT"
   },
   "source": [
    "Next we'll pull 500 rows of values from our new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-DROr3veo3J"
   },
   "outputs": [],
   "source": [
    "words = web_pages[\"stopwords\"].head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgiOIFUU9_3I"
   },
   "source": [
    "Now we can create a word cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "2Kp0xnU1eud6",
    "outputId": "1cc7a45f-f60f-4f1c-817d-d671f3e88a48"
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    width=2000,\n",
    "    height=1500,\n",
    "    scale=10,\n",
    "    max_font_size=250,\n",
    "    max_words=100,\n",
    "    background_color=\"white\",\n",
    ").generate(str(words))\n",
    "plt.figure(figsize=[35, 10])\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "arch-example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
